# AI Evaluation in the Development Sector  
*A Living Playbook by The Agency Fund*  
**ðŸš§ Draft in Progress â€“ For Internal Use Only ðŸš§**

---

## Overview

> "Evals are surprisingly often all you need."  
> â€” *Greg Brockman, Co-founder of OpenAI*

We agree â€” **but only if weâ€™re clear on what â€œevaluationâ€ really means.**

This playbook is designed for practitioners, funders, and policymakers working at the frontier of **Generative AI (GenAI)** in the development sector. It offers a practical, structured approach to AI evaluation that goes beyond model performance â€” focusing instead on **real, measurable impact on peopleâ€™s lives**.

This resource is maintained by [The Agency Fund](https://agencyfund.org) as part of our broader effort to make AI more equitable, effective, and accountable in global development.

---

## ðŸŒ Why This Playbook?

In 2025, The Agency Fund launched the **AI for Global Development (AI4GD)** accelerator in collaboration with [OpenAI](https://openai.com) and the [Center for Global Development (CGD)](https://cgdev.org). The goal: support high-impact GenAI solutions across **education, health, and agriculture** â€” and learn how to evaluate them meaningfully.

Despite strong interest in â€œevaluation,â€ we found no shared playbook. Social sector actors are using a patchwork of evaluation tools â€” often defaulting to RCTs when they may not be appropriate. Our playbook addresses this gap with a structured, adaptable framework.

---

## ðŸ§­ The Four-Level Evaluation Framework

We introduce a **four-level evaluation framework** to clarify what AI evaluation means across product maturity stages:

| Level | Key Question                                           | Example Stakeholders                   |
|-------|--------------------------------------------------------|----------------------------------------|
| 1     | *Model Evaluation:* Does the AI model perform as expected? | AI engineers                            |
| 2     | *Product Evaluation:* Do users engage meaningfully with the product? | Product managers, data scientists       |
| 3     | *User Evaluation:* Does the product change user beliefs or behavior? | Behavioral scientists, UX researchers   |
| 4     | *Impact Evaluation:* Does it improve development outcomes? | Economists, policymakers                |

Each level builds on the last â€” but requires **distinct methods, skillsets, and tradeoffs.**

---

## âš ï¸ Guiding Principles

1. **Cross-functional collaboration is essential** â€” engineers, researchers, and implementers must align.
2. **Higher levels mean more cost and risk** â€” but also higher potential for impact.
3. **Evaluation informs but does not replace product design** â€” user research, content strategy, and design still matter.

---

## ðŸ“š Whatâ€™s in the Playbook?

- A detailed explanation of the four levels
- Examples from the AI4GD accelerator
- Practical tools and decision guides for each level
- Future directions for methodology development

> This is **not** a full product development guide â€” but it pairs with our upcoming playbooks on **user research**, **product design**, and **experimentation**.

---

## ðŸš§ Current Status

This playbook is a **living document**. Current content is based on:
- Lessons from 8 GenAI projects across health, education, and agriculture
- Field insights from AI4GD implementation
- Ongoing input from behavioral researchers, economists, and technical experts

ðŸ”„ **Next steps:** Weâ€™re working to co-develop shared evaluation tools, refine methods, and publish field-tested guidance.

---

## ðŸ“¬ Contact

Have suggestions, ideas, or want to collaborate?  
Please reach out to the authors via [email](https://eval.playbook.org.ai/authors).

---

## License

This project is shared for internal and collaborative use only. External sharing or reproduction is not permitted without explicit permission from The Agency Fund.

---

Â© The Agency Fund â€“ July 2025
