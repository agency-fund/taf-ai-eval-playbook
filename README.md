# AI Evaluation in the Development Sector  
*A Living Playbook by The Agency Fund*  
**🚧 Draft in Progress – For Internal Use Only 🚧**

---

## Overview

> "Evals are surprisingly often all you need."  
> — *Greg Brockman, Co-founder of OpenAI*

We agree — **but only if we’re clear on what “evaluation” really means.**

This playbook is designed for practitioners, funders, and policymakers working at the frontier of **Generative AI (GenAI)** in the development sector. It offers a practical, structured approach to AI evaluation that goes beyond model performance — focusing instead on **real, measurable impact on people’s lives**.

This resource is maintained by [The Agency Fund](https://agencyfund.org) as part of our broader effort to make AI more equitable, effective, and accountable in global development.

---

## 🌍 Why This Playbook?

In 2025, The Agency Fund launched the **AI for Global Development (AI4GD)** accelerator in collaboration with [OpenAI](https://openai.com) and the [Center for Global Development (CGD)](https://cgdev.org). The goal: support high-impact GenAI solutions across **education, health, and agriculture** — and learn how to evaluate them meaningfully.

Despite strong interest in “evaluation,” we found no shared playbook. Social sector actors are using a patchwork of evaluation tools — often defaulting to RCTs when they may not be appropriate. Our playbook addresses this gap with a structured, adaptable framework.

---

## 🧭 The Four-Level Evaluation Framework

We introduce a **four-level evaluation framework** to clarify what AI evaluation means across product maturity stages:

| Level | Key Question                                           | Example Stakeholders                   |
|-------|--------------------------------------------------------|----------------------------------------|
| 1     | *Model Evaluation:* Does the AI model perform as expected? | AI engineers                            |
| 2     | *Product Evaluation:* Do users engage meaningfully with the product? | Product managers, data scientists       |
| 3     | *User Evaluation:* Does the product change user beliefs or behavior? | Behavioral scientists, UX researchers   |
| 4     | *Impact Evaluation:* Does it improve development outcomes? | Economists, policymakers                |

Each level builds on the last — but requires **distinct methods, skillsets, and tradeoffs.**

---

## ⚠️ Guiding Principles

1. **Cross-functional collaboration is essential** — engineers, researchers, and implementers must align.
2. **Higher levels mean more cost and risk** — but also higher potential for impact.
3. **Evaluation informs but does not replace product design** — user research, content strategy, and design still matter.

---

## 📚 What’s in the Playbook?

- A detailed explanation of the four levels
- Examples from the AI4GD accelerator
- Practical tools and decision guides for each level
- Future directions for methodology development

> This is **not** a full product development guide — but it pairs with our upcoming playbooks on **user research**, **product design**, and **experimentation**.

---

## 🚧 Current Status

This playbook is a **living document**. Current content is based on:
- Lessons from 8 GenAI projects across health, education, and agriculture
- Field insights from AI4GD implementation
- Ongoing input from behavioral researchers, economists, and technical experts

🔄 **Next steps:** We’re working to co-develop shared evaluation tools, refine methods, and publish field-tested guidance.

---

## 📬 Contact

Have suggestions, ideas, or want to collaborate?  
Please reach out to the authors via [email](https://eval.playbook.org.ai/authors).

---

## License

This project is shared for internal and collaborative use only. External sharing or reproduction is not permitted without explicit permission from The Agency Fund.

---

© The Agency Fund – July 2025
